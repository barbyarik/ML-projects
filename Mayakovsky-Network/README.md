# Нейронная сеть "Маяковский"

Проект, демонстрирующий задачу генерации текста. Эта задача не нова, поэтому ради интереса создаваемые тексты 
обладают побочным условием — они должны представлять собой стихи в стилистике Владимира Маяковского. Для 
решение задачи применялось два подхода: рекуррентная нейронная сеть, обученная на выборке стихотворений поэта 
в ~ $400.000$ символов; и предобученная языковая модель ```GPT-Neo``` от ```HuggingFace```.

## Рекуррентная нейросеть

* Архитектура: ```LSTM-слой``` на 128 нейронов -> ```LSTM-слой``` на 128 нейронов -> ```Dense-слой``` (обычный 
полносвязный) -> ```Softmax-слой``` (для получения вероятностей символов);
* Количество эпох в обучении: 100;
* Материал для обучения: ```data/Mayakovsky.txt```.

По окончании обучения получены результаты, подобные этим:

    а теле твоем - как на смертном одре сердца разом!

    товарищ, не отоброй,

    и стол

    и к неба к нам. да как думатывник в опертяльхы прочист жил были. но в тунары

    и пролезанных нам. последние

    в кабрумая род-то вычестно, котчит рьскиу

Можно заметить, что модель переняла тенденцию писать "лесенкой", однако испытывает трудности с грамматикой и 
осмысленностью текста. Вероятно, это получилось ввиду обучения на задаче предсказания символа, а не целого слова, 
ограниченности обучающей выборки и количества обучаемых параметров.

## GPT-Neo

Аналогичная задча была передана уже предобученному трансформеру ```GPT-Neo``` с 125М параметров от ```HuggingFace```. 
Изменились ли результаты? Вот пример текста, генерируемого ```GPT-Neo```:

```
Владимир Маяковский написал новый стих. И звучит он так:
Был теплый зимний вечер, определенный в корректности написал стиховый шарлерный страничкой видом (для операции этой звучит нового инвестора двигательного). Дальше распахны воин словами (у нас инвестором написал виновна сберу горизонта Маяковской стихов в Альии) до общемистков маходов (написаться, и позволяюще...
```

Что ж, теперь слова являются грамматически правильными и появляется какая-никакая связь между ними. Но отсутствует стиль поэта и уровень осмысленности 
всё ещё низок. Вероятно, лучших результатов можно добиться, если использовать более глубокие модели с большим количеством обучаемых 
параметров, например, ```ruGPT-3.5``` на 13B параметров.